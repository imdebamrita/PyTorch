{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2MbQg+x9jXVUe1HLHahuF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":26,"metadata":{"id":"0bX1dhQ7YN9H","executionInfo":{"status":"ok","timestamp":1721049319371,"user_tz":-330,"elapsed":704,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"outputs":[],"source":["import numpy as np\n","import torch"]},{"cell_type":"code","source":["X = np.array([1, 2, 3, 4], dtype=np.float32)\n","Y = np.array([2, 4, 6, 8], dtype=np.float32)"],"metadata":{"id":"xNpXtrtaYTk3","executionInfo":{"status":"ok","timestamp":1721047608288,"user_tz":-330,"elapsed":456,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["w = 0.0"],"metadata":{"id":"DG7k4o0bbmdu","executionInfo":{"status":"ok","timestamp":1721048294277,"user_tz":-330,"elapsed":482,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# model prediction\n","def forward(x):\n","  return w * x"],"metadata":{"id":"nEM1McDVbpB-","executionInfo":{"status":"ok","timestamp":1721049408182,"user_tz":-330,"elapsed":452,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# loss\n","def loss(y, y_predicted):\n","  return ((y_predicted - y)**2).mean()"],"metadata":{"id":"PQ4QcDSDbsHV","executionInfo":{"status":"ok","timestamp":1721047918029,"user_tz":-330,"elapsed":2,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["#gradient\n","\n","def gradient(x, y, y_predicted):\n","  return np.dot(2*x, y_predicted-y).mean()"],"metadata":{"id":"DRUZn6PHb-Aw","executionInfo":{"status":"ok","timestamp":1721047918030,"user_tz":-330,"elapsed":2,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["print(f\"Predction before tranning: f(5) = {forward(5):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jwp4iLQfcBLh","executionInfo":{"status":"ok","timestamp":1721048231211,"user_tz":-330,"elapsed":505,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}},"outputId":"d9d53589-a008-4502-ae38-8e7d189bd07d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Predction before tranning: f(5) = 0.000\n"]}]},{"cell_type":"code","source":["#Traning\n","\n","learning_rate = 0.01\n","n_iters = 10\n","\n","for epoch in range(n_iters):\n","  # prediction = forward pass\n","  y_pred = forward(X)\n","\n","  #loss\n","  l = loss(Y, y_pred)\n","\n","  #gradients\n","  dw = gradient(X, Y, y_pred)\n","\n","  #updata weights\n","  w -= learning_rate * dw\n","\n","  if epoch % 1 == 0:\n","    print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.3f}\")\n","\n","print(f\"Predction after tranning: f(5) = {forward(5):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3Q6aoipcueN","executionInfo":{"status":"ok","timestamp":1721048298451,"user_tz":-330,"elapsed":470,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}},"outputId":"65e6cf4a-7fe6-479b-e7fd-9b162375bf11"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1: w = 1.200, loss = 30.000\n","epoch 2: w = 1.680, loss = 4.800\n","epoch 3: w = 1.872, loss = 0.768\n","epoch 4: w = 1.949, loss = 0.123\n","epoch 5: w = 1.980, loss = 0.020\n","epoch 6: w = 1.992, loss = 0.003\n","epoch 7: w = 1.997, loss = 0.001\n","epoch 8: w = 1.999, loss = 0.000\n","epoch 9: w = 1.999, loss = 0.000\n","epoch 10: w = 2.000, loss = 0.000\n","Predction after tranning: f(5) = 9.999\n"]}]},{"cell_type":"code","source":["X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n","Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)"],"metadata":{"id":"MsaPjaY-c1xM","executionInfo":{"status":"ok","timestamp":1721050264482,"user_tz":-330,"elapsed":559,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)"],"metadata":{"id":"lkh7q6MjiGpN","executionInfo":{"status":"ok","timestamp":1721050315589,"user_tz":-330,"elapsed":8,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# model prediction\n","def forward(x):\n","  return w * x"],"metadata":{"id":"MrLjKbLWjYaG","executionInfo":{"status":"ok","timestamp":1721050315590,"user_tz":-330,"elapsed":8,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# loss\n","def loss(y, y_predicted):\n","  return ((y_predicted - y)**2).mean()"],"metadata":{"id":"c2h1Vkiwjc4g","executionInfo":{"status":"ok","timestamp":1721050315590,"user_tz":-330,"elapsed":7,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["print(f\"Predction before tranning: f(5) = {forward(5):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8k2-W7SpjgIP","executionInfo":{"status":"ok","timestamp":1721050315590,"user_tz":-330,"elapsed":7,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}},"outputId":"90e340cf-0152-4be0-d0f3-2fd00e7e905f"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Predction before tranning: f(5) = 0.000\n"]}]},{"cell_type":"code","source":["#Traning\n","\n","learning_rate = 0.01\n","n_iters = 100\n","\n","for epoch in range(n_iters):\n","  # prediction = forward pass\n","  y_pred = forward(X)\n","\n","  #loss\n","  l = loss(Y, y_pred)\n","\n","  #gradients = backward pass\n","  l.backward() # dl/dw\n","\n","  #updata weights\n","  with torch.no_grad():\n","    w -= learning_rate * w.grad\n","\n","  #zero gradients\n","  w.grad.zero_()\n","\n","  if epoch % 1 == 0:\n","    print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.3f}\")\n","\n","print(f\"Predction after tranning: f(5) = {forward(5):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JvJpYjPjkem","executionInfo":{"status":"ok","timestamp":1721050315590,"user_tz":-330,"elapsed":5,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}},"outputId":"fdf8e11e-1621-4479-eaad-60970c0890fb"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1: w = 0.300, loss = 30.000\n","epoch 2: w = 0.555, loss = 21.675\n","epoch 3: w = 0.772, loss = 15.660\n","epoch 4: w = 0.956, loss = 11.314\n","epoch 5: w = 1.113, loss = 8.175\n","epoch 6: w = 1.246, loss = 5.906\n","epoch 7: w = 1.359, loss = 4.267\n","epoch 8: w = 1.455, loss = 3.083\n","epoch 9: w = 1.537, loss = 2.228\n","epoch 10: w = 1.606, loss = 1.609\n","epoch 11: w = 1.665, loss = 1.163\n","epoch 12: w = 1.716, loss = 0.840\n","epoch 13: w = 1.758, loss = 0.607\n","epoch 14: w = 1.794, loss = 0.439\n","epoch 15: w = 1.825, loss = 0.317\n","epoch 16: w = 1.851, loss = 0.229\n","epoch 17: w = 1.874, loss = 0.165\n","epoch 18: w = 1.893, loss = 0.119\n","epoch 19: w = 1.909, loss = 0.086\n","epoch 20: w = 1.922, loss = 0.062\n","epoch 21: w = 1.934, loss = 0.045\n","epoch 22: w = 1.944, loss = 0.033\n","epoch 23: w = 1.952, loss = 0.024\n","epoch 24: w = 1.960, loss = 0.017\n","epoch 25: w = 1.966, loss = 0.012\n","epoch 26: w = 1.971, loss = 0.009\n","epoch 27: w = 1.975, loss = 0.006\n","epoch 28: w = 1.979, loss = 0.005\n","epoch 29: w = 1.982, loss = 0.003\n","epoch 30: w = 1.985, loss = 0.002\n","epoch 31: w = 1.987, loss = 0.002\n","epoch 32: w = 1.989, loss = 0.001\n","epoch 33: w = 1.991, loss = 0.001\n","epoch 34: w = 1.992, loss = 0.001\n","epoch 35: w = 1.993, loss = 0.000\n","epoch 36: w = 1.994, loss = 0.000\n","epoch 37: w = 1.995, loss = 0.000\n","epoch 38: w = 1.996, loss = 0.000\n","epoch 39: w = 1.996, loss = 0.000\n","epoch 40: w = 1.997, loss = 0.000\n","epoch 41: w = 1.997, loss = 0.000\n","epoch 42: w = 1.998, loss = 0.000\n","epoch 43: w = 1.998, loss = 0.000\n","epoch 44: w = 1.998, loss = 0.000\n","epoch 45: w = 1.999, loss = 0.000\n","epoch 46: w = 1.999, loss = 0.000\n","epoch 47: w = 1.999, loss = 0.000\n","epoch 48: w = 1.999, loss = 0.000\n","epoch 49: w = 1.999, loss = 0.000\n","epoch 50: w = 1.999, loss = 0.000\n","epoch 51: w = 1.999, loss = 0.000\n","epoch 52: w = 2.000, loss = 0.000\n","epoch 53: w = 2.000, loss = 0.000\n","epoch 54: w = 2.000, loss = 0.000\n","epoch 55: w = 2.000, loss = 0.000\n","epoch 56: w = 2.000, loss = 0.000\n","epoch 57: w = 2.000, loss = 0.000\n","epoch 58: w = 2.000, loss = 0.000\n","epoch 59: w = 2.000, loss = 0.000\n","epoch 60: w = 2.000, loss = 0.000\n","epoch 61: w = 2.000, loss = 0.000\n","epoch 62: w = 2.000, loss = 0.000\n","epoch 63: w = 2.000, loss = 0.000\n","epoch 64: w = 2.000, loss = 0.000\n","epoch 65: w = 2.000, loss = 0.000\n","epoch 66: w = 2.000, loss = 0.000\n","epoch 67: w = 2.000, loss = 0.000\n","epoch 68: w = 2.000, loss = 0.000\n","epoch 69: w = 2.000, loss = 0.000\n","epoch 70: w = 2.000, loss = 0.000\n","epoch 71: w = 2.000, loss = 0.000\n","epoch 72: w = 2.000, loss = 0.000\n","epoch 73: w = 2.000, loss = 0.000\n","epoch 74: w = 2.000, loss = 0.000\n","epoch 75: w = 2.000, loss = 0.000\n","epoch 76: w = 2.000, loss = 0.000\n","epoch 77: w = 2.000, loss = 0.000\n","epoch 78: w = 2.000, loss = 0.000\n","epoch 79: w = 2.000, loss = 0.000\n","epoch 80: w = 2.000, loss = 0.000\n","epoch 81: w = 2.000, loss = 0.000\n","epoch 82: w = 2.000, loss = 0.000\n","epoch 83: w = 2.000, loss = 0.000\n","epoch 84: w = 2.000, loss = 0.000\n","epoch 85: w = 2.000, loss = 0.000\n","epoch 86: w = 2.000, loss = 0.000\n","epoch 87: w = 2.000, loss = 0.000\n","epoch 88: w = 2.000, loss = 0.000\n","epoch 89: w = 2.000, loss = 0.000\n","epoch 90: w = 2.000, loss = 0.000\n","epoch 91: w = 2.000, loss = 0.000\n","epoch 92: w = 2.000, loss = 0.000\n","epoch 93: w = 2.000, loss = 0.000\n","epoch 94: w = 2.000, loss = 0.000\n","epoch 95: w = 2.000, loss = 0.000\n","epoch 96: w = 2.000, loss = 0.000\n","epoch 97: w = 2.000, loss = 0.000\n","epoch 98: w = 2.000, loss = 0.000\n","epoch 99: w = 2.000, loss = 0.000\n","epoch 100: w = 2.000, loss = 0.000\n","Predction after tranning: f(5) = 10.000\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"KtDnL_gwlYVP","executionInfo":{"status":"ok","timestamp":1721050265697,"user_tz":-330,"elapsed":3,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":39,"outputs":[]}]}