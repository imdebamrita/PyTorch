{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMnw6NQNHUSq7Y0AvWmuQ67"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","## PyTorch Model Pipeline\n","1.   Design model (input, output size, forward pass)\n","2.   Construct loss and optimizer\n","3.   Traning loop\n","    - forward pass: compute prediction\n","    - backward pass: gradients\n","    - update weights\n","\n","\n"],"metadata":{"id":"-YPdcGygVMBe"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"WB2pE8bWV-jg","executionInfo":{"status":"ok","timestamp":1721062966569,"user_tz":-330,"elapsed":641,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n","Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n","\n","w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)"],"metadata":{"id":"WNMR4-BGWLyT","executionInfo":{"status":"ok","timestamp":1721063734794,"user_tz":-330,"elapsed":3,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def forward(x):\n","  return w * x"],"metadata":{"id":"Z3Vgf5RKWa2V","executionInfo":{"status":"ok","timestamp":1721063169242,"user_tz":-330,"elapsed":471,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(f\"Predction before tranning: f(5) = {forward(5):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kv4idpIjW9gg","executionInfo":{"status":"ok","timestamp":1721063267671,"user_tz":-330,"elapsed":462,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}},"outputId":"0d10dc4b-2807-4e90-8c84-820d2baaa465"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Predction before tranning: f(5) = 0.000\n"]}]},{"cell_type":"code","source":["learning_rate = 0.01\n","n_iters = 100\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD([w], lr=learning_rate)\n","\n","for epoch in range(n_iters):\n","  y_pred = forward(X)\n","\n","  l = loss(Y, y_pred)\n","\n","  l.backward()\n","\n","  optimizer.step()\n","\n","  optimizer.zero_grad()\n","\n","  if epoch % 10 == 0:\n","    print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n","\n","print(f\"Prediction after traning: f(5) = {forward(5):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22bvphdlXVi1","executionInfo":{"status":"ok","timestamp":1721063562406,"user_tz":-330,"elapsed":2730,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}},"outputId":"96286445-caa5-49f5-805c-54f92af19aa2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1: w = 0.300, loss = 30.00000000\n","epoch 11: w = 1.665, loss = 1.16278565\n","epoch 21: w = 1.934, loss = 0.04506890\n","epoch 31: w = 1.987, loss = 0.00174685\n","epoch 41: w = 1.997, loss = 0.00006770\n","epoch 51: w = 1.999, loss = 0.00000262\n","epoch 61: w = 2.000, loss = 0.00000010\n","epoch 71: w = 2.000, loss = 0.00000000\n","epoch 81: w = 2.000, loss = 0.00000000\n","epoch 91: w = 2.000, loss = 0.00000000\n","Prediction after traning: f(5) = 10.000\n"]}]},{"cell_type":"code","source":["X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n","Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n","\n","X_test = torch.tensor([5], dtype=torch.float32)\n","\n","n_samples, n_features = X.shape\n","print(n_samples, n_features)\n","\n","input_size = n_features\n","output_size = n_features\n","\n","model = nn.Linear(input_size, output_size)\n","\n","print(f\"Predction before tranning: f(5) = {model(X_test).item():.3f}\")\n","\n","learning_rate = 0.05\n","n_iters = 1000\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(n_iters):\n","  y_pred = model(X)\n","\n","  l = loss(Y, y_pred)\n","\n","  l.backward()\n","\n","  optimizer.step()\n","\n","  optimizer.zero_grad()\n","\n","  if epoch % 10 == 0:\n","    [w, b] = model.parameters()\n","    print(f\"epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}\")\n","\n","print(f\"Prediction after traning: f(5) = {model(X_test).item():.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHUEteMLYc7t","executionInfo":{"status":"ok","timestamp":1721064360005,"user_tz":-330,"elapsed":963,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}},"outputId":"a700c09a-fcbc-4f8d-ae2f-e32624dfa042"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["4 1\n","Predction before tranning: f(5) = -2.242\n","epoch 1: w = 1.281, loss = 43.24656677\n","epoch 11: w = 1.686, loss = 0.14642188\n","epoch 21: w = 1.730, loss = 0.10829279\n","epoch 31: w = 1.768, loss = 0.08009275\n","epoch 41: w = 1.800, loss = 0.05923615\n","epoch 51: w = 1.828, loss = 0.04381069\n","epoch 61: w = 1.852, loss = 0.03240214\n","epoch 71: w = 1.873, loss = 0.02396440\n","epoch 81: w = 1.891, loss = 0.01772396\n","epoch 91: w = 1.906, loss = 0.01310853\n","epoch 101: w = 1.919, loss = 0.00969498\n","epoch 111: w = 1.931, loss = 0.00717037\n","epoch 121: w = 1.940, loss = 0.00530316\n","epoch 131: w = 1.949, loss = 0.00392218\n","epoch 141: w = 1.956, loss = 0.00290082\n","epoch 151: w = 1.962, loss = 0.00214543\n","epoch 161: w = 1.967, loss = 0.00158675\n","epoch 171: w = 1.972, loss = 0.00117355\n","epoch 181: w = 1.976, loss = 0.00086796\n","epoch 191: w = 1.979, loss = 0.00064194\n","epoch 201: w = 1.982, loss = 0.00047477\n","epoch 211: w = 1.985, loss = 0.00035114\n","epoch 221: w = 1.987, loss = 0.00025970\n","epoch 231: w = 1.989, loss = 0.00019207\n","epoch 241: w = 1.990, loss = 0.00014206\n","epoch 251: w = 1.992, loss = 0.00010506\n","epoch 261: w = 1.993, loss = 0.00007770\n","epoch 271: w = 1.994, loss = 0.00005747\n","epoch 281: w = 1.995, loss = 0.00004250\n","epoch 291: w = 1.995, loss = 0.00003144\n","epoch 301: w = 1.996, loss = 0.00002325\n","epoch 311: w = 1.997, loss = 0.00001720\n","epoch 321: w = 1.997, loss = 0.00001272\n","epoch 331: w = 1.997, loss = 0.00000941\n","epoch 341: w = 1.998, loss = 0.00000696\n","epoch 351: w = 1.998, loss = 0.00000515\n","epoch 361: w = 1.998, loss = 0.00000381\n","epoch 371: w = 1.999, loss = 0.00000281\n","epoch 381: w = 1.999, loss = 0.00000208\n","epoch 391: w = 1.999, loss = 0.00000154\n","epoch 401: w = 1.999, loss = 0.00000114\n","epoch 411: w = 1.999, loss = 0.00000084\n","epoch 421: w = 1.999, loss = 0.00000062\n","epoch 431: w = 1.999, loss = 0.00000046\n","epoch 441: w = 2.000, loss = 0.00000034\n","epoch 451: w = 2.000, loss = 0.00000025\n","epoch 461: w = 2.000, loss = 0.00000019\n","epoch 471: w = 2.000, loss = 0.00000014\n","epoch 481: w = 2.000, loss = 0.00000010\n","epoch 491: w = 2.000, loss = 0.00000008\n","epoch 501: w = 2.000, loss = 0.00000006\n","epoch 511: w = 2.000, loss = 0.00000004\n","epoch 521: w = 2.000, loss = 0.00000003\n","epoch 531: w = 2.000, loss = 0.00000002\n","epoch 541: w = 2.000, loss = 0.00000002\n","epoch 551: w = 2.000, loss = 0.00000001\n","epoch 561: w = 2.000, loss = 0.00000001\n","epoch 571: w = 2.000, loss = 0.00000001\n","epoch 581: w = 2.000, loss = 0.00000000\n","epoch 591: w = 2.000, loss = 0.00000000\n","epoch 601: w = 2.000, loss = 0.00000000\n","epoch 611: w = 2.000, loss = 0.00000000\n","epoch 621: w = 2.000, loss = 0.00000000\n","epoch 631: w = 2.000, loss = 0.00000000\n","epoch 641: w = 2.000, loss = 0.00000000\n","epoch 651: w = 2.000, loss = 0.00000000\n","epoch 661: w = 2.000, loss = 0.00000000\n","epoch 671: w = 2.000, loss = 0.00000000\n","epoch 681: w = 2.000, loss = 0.00000000\n","epoch 691: w = 2.000, loss = 0.00000000\n","epoch 701: w = 2.000, loss = 0.00000000\n","epoch 711: w = 2.000, loss = 0.00000000\n","epoch 721: w = 2.000, loss = 0.00000000\n","epoch 731: w = 2.000, loss = 0.00000000\n","epoch 741: w = 2.000, loss = 0.00000000\n","epoch 751: w = 2.000, loss = 0.00000000\n","epoch 761: w = 2.000, loss = 0.00000000\n","epoch 771: w = 2.000, loss = 0.00000000\n","epoch 781: w = 2.000, loss = 0.00000000\n","epoch 791: w = 2.000, loss = 0.00000000\n","epoch 801: w = 2.000, loss = 0.00000000\n","epoch 811: w = 2.000, loss = 0.00000000\n","epoch 821: w = 2.000, loss = 0.00000000\n","epoch 831: w = 2.000, loss = 0.00000000\n","epoch 841: w = 2.000, loss = 0.00000000\n","epoch 851: w = 2.000, loss = 0.00000000\n","epoch 861: w = 2.000, loss = 0.00000000\n","epoch 871: w = 2.000, loss = 0.00000000\n","epoch 881: w = 2.000, loss = 0.00000000\n","epoch 891: w = 2.000, loss = 0.00000000\n","epoch 901: w = 2.000, loss = 0.00000000\n","epoch 911: w = 2.000, loss = 0.00000000\n","epoch 921: w = 2.000, loss = 0.00000000\n","epoch 931: w = 2.000, loss = 0.00000000\n","epoch 941: w = 2.000, loss = 0.00000000\n","epoch 951: w = 2.000, loss = 0.00000000\n","epoch 961: w = 2.000, loss = 0.00000000\n","epoch 971: w = 2.000, loss = 0.00000000\n","epoch 981: w = 2.000, loss = 0.00000000\n","epoch 991: w = 2.000, loss = 0.00000000\n","Prediction after traning: f(5) = 10.000\n"]}]},{"cell_type":"code","source":["X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n","Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n","\n","X_test = torch.tensor([5], dtype=torch.float32)\n","\n","n_samples, n_features = X.shape\n","print(n_samples, n_features)\n","\n","input_size = n_features\n","output_size = n_features\n","\n","# model = nn.Linear(input_size, output_size)\n","class LinearRegression(nn.Module):\n","  def __init__(self, input_dim, output_dim):\n","    super(LinearRegression, self).__init__()\n","    # define layers\n","    self.lin = nn.Linear(input_dim, output_dim)\n","\n","  def forward(self, x):\n","    return self.lin(x)\n","\n","model = LinearRegression(input_size, output_size)\n","\n","print(f\"Predction before tranning: f(5) = {model(X_test).item():.3f}\")\n","\n","learning_rate = 0.05\n","n_iters = 1000\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(n_iters):\n","  y_pred = model(X)\n","\n","  l = loss(Y, y_pred)\n","\n","  l.backward()\n","\n","  optimizer.step()\n","\n","  optimizer.zero_grad()\n","\n","  if epoch % 10 == 0:\n","    [w, b] = model.parameters()\n","    print(f\"epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}\")\n","\n","print(f\"Prediction after traning: f(5) = {model(X_test).item():.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1u7Pyk12ZZzT","executionInfo":{"status":"ok","timestamp":1721064701941,"user_tz":-330,"elapsed":1194,"user":{"displayName":"Debamrita Paul","userId":"17981460538508157461"}},"outputId":"c37b5282-f544-4847-dce2-ae910a8645f8"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["4 1\n","Predction before tranning: f(5) = -1.651\n","epoch 1: w = 1.204, loss = 37.55161285\n","epoch 11: w = 1.599, loss = 0.23960103\n","epoch 21: w = 1.655, loss = 0.17720747\n","epoch 31: w = 1.703, loss = 0.13106170\n","epoch 41: w = 1.745, loss = 0.09693243\n","epoch 51: w = 1.780, loss = 0.07169063\n","epoch 61: w = 1.811, loss = 0.05302195\n","epoch 71: w = 1.838, loss = 0.03921477\n","epoch 81: w = 1.860, loss = 0.02900300\n","epoch 91: w = 1.880, loss = 0.02145044\n","epoch 101: w = 1.897, loss = 0.01586459\n","epoch 111: w = 1.911, loss = 0.01173337\n","epoch 121: w = 1.924, loss = 0.00867793\n","epoch 131: w = 1.934, loss = 0.00641815\n","epoch 141: w = 1.944, loss = 0.00474683\n","epoch 151: w = 1.951, loss = 0.00351073\n","epoch 161: w = 1.958, loss = 0.00259651\n","epoch 171: w = 1.964, loss = 0.00192036\n","epoch 181: w = 1.969, loss = 0.00142029\n","epoch 191: w = 1.973, loss = 0.00105044\n","epoch 201: w = 1.977, loss = 0.00077690\n","epoch 211: w = 1.980, loss = 0.00057459\n","epoch 221: w = 1.983, loss = 0.00042496\n","epoch 231: w = 1.985, loss = 0.00031430\n","epoch 241: w = 1.988, loss = 0.00023246\n","epoch 251: w = 1.989, loss = 0.00017192\n","epoch 261: w = 1.991, loss = 0.00012715\n","epoch 271: w = 1.992, loss = 0.00009404\n","epoch 281: w = 1.993, loss = 0.00006955\n","epoch 291: w = 1.994, loss = 0.00005144\n","epoch 301: w = 1.995, loss = 0.00003804\n","epoch 311: w = 1.996, loss = 0.00002814\n","epoch 321: w = 1.996, loss = 0.00002081\n","epoch 331: w = 1.997, loss = 0.00001539\n","epoch 341: w = 1.997, loss = 0.00001138\n","epoch 351: w = 1.998, loss = 0.00000842\n","epoch 361: w = 1.998, loss = 0.00000623\n","epoch 371: w = 1.998, loss = 0.00000461\n","epoch 381: w = 1.998, loss = 0.00000341\n","epoch 391: w = 1.999, loss = 0.00000252\n","epoch 401: w = 1.999, loss = 0.00000186\n","epoch 411: w = 1.999, loss = 0.00000138\n","epoch 421: w = 1.999, loss = 0.00000102\n","epoch 431: w = 1.999, loss = 0.00000075\n","epoch 441: w = 1.999, loss = 0.00000056\n","epoch 451: w = 1.999, loss = 0.00000041\n","epoch 461: w = 2.000, loss = 0.00000030\n","epoch 471: w = 2.000, loss = 0.00000023\n","epoch 481: w = 2.000, loss = 0.00000017\n","epoch 491: w = 2.000, loss = 0.00000012\n","epoch 501: w = 2.000, loss = 0.00000009\n","epoch 511: w = 2.000, loss = 0.00000007\n","epoch 521: w = 2.000, loss = 0.00000005\n","epoch 531: w = 2.000, loss = 0.00000004\n","epoch 541: w = 2.000, loss = 0.00000003\n","epoch 551: w = 2.000, loss = 0.00000002\n","epoch 561: w = 2.000, loss = 0.00000001\n","epoch 571: w = 2.000, loss = 0.00000001\n","epoch 581: w = 2.000, loss = 0.00000001\n","epoch 591: w = 2.000, loss = 0.00000001\n","epoch 601: w = 2.000, loss = 0.00000000\n","epoch 611: w = 2.000, loss = 0.00000000\n","epoch 621: w = 2.000, loss = 0.00000000\n","epoch 631: w = 2.000, loss = 0.00000000\n","epoch 641: w = 2.000, loss = 0.00000000\n","epoch 651: w = 2.000, loss = 0.00000000\n","epoch 661: w = 2.000, loss = 0.00000000\n","epoch 671: w = 2.000, loss = 0.00000000\n","epoch 681: w = 2.000, loss = 0.00000000\n","epoch 691: w = 2.000, loss = 0.00000000\n","epoch 701: w = 2.000, loss = 0.00000000\n","epoch 711: w = 2.000, loss = 0.00000000\n","epoch 721: w = 2.000, loss = 0.00000000\n","epoch 731: w = 2.000, loss = 0.00000000\n","epoch 741: w = 2.000, loss = 0.00000000\n","epoch 751: w = 2.000, loss = 0.00000000\n","epoch 761: w = 2.000, loss = 0.00000000\n","epoch 771: w = 2.000, loss = 0.00000000\n","epoch 781: w = 2.000, loss = 0.00000000\n","epoch 791: w = 2.000, loss = 0.00000000\n","epoch 801: w = 2.000, loss = 0.00000000\n","epoch 811: w = 2.000, loss = 0.00000000\n","epoch 821: w = 2.000, loss = 0.00000000\n","epoch 831: w = 2.000, loss = 0.00000000\n","epoch 841: w = 2.000, loss = 0.00000000\n","epoch 851: w = 2.000, loss = 0.00000000\n","epoch 861: w = 2.000, loss = 0.00000000\n","epoch 871: w = 2.000, loss = 0.00000000\n","epoch 881: w = 2.000, loss = 0.00000000\n","epoch 891: w = 2.000, loss = 0.00000000\n","epoch 901: w = 2.000, loss = 0.00000000\n","epoch 911: w = 2.000, loss = 0.00000000\n","epoch 921: w = 2.000, loss = 0.00000000\n","epoch 931: w = 2.000, loss = 0.00000000\n","epoch 941: w = 2.000, loss = 0.00000000\n","epoch 951: w = 2.000, loss = 0.00000000\n","epoch 961: w = 2.000, loss = 0.00000000\n","epoch 971: w = 2.000, loss = 0.00000000\n","epoch 981: w = 2.000, loss = 0.00000000\n","epoch 991: w = 2.000, loss = 0.00000000\n","Prediction after traning: f(5) = 10.000\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4ehsNhi6cLtt"},"execution_count":null,"outputs":[]}]}